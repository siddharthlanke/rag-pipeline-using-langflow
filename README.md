# RAG Pipeline Using Langflow  

A **Retrieval-Augmented Generation (RAG)** pipeline built using Langflow, Astra DB, Ollama embeddings, and the Llama3.2 LLM. This project demonstrates document ingestion, vector storage, and context-aware question answering using a custom PDF story (*Shadows of Eldoria*).

## üîë Key Components  

1. **Document**: *Shadows of Eldoria.pdf* (3,000-word fictional story generated by ChatGPT).  
2. **Vector Database**: **Astra DB** (by DataStax) for storing embeddings.  
3. **Embedding Model**: **Ollama's `all-minilm:latest`** (384 dimensions).  
4. **LLM**: **Llama3.2** (hosted locally via Ollama).  
5. **Tools**: Built with [Langflow](https://github.com/logspace-ai/langflow).  

## ‚öôÔ∏è Pipeline Workflow  

1. **Document Ingestion**  
- Extract text from the PDF and split it into chunks:  
  - `chunk_size = 500`  
  - `chunk_overlap = 100`  
2. **Embedding Generation**  
- Convert text chunks into **384-dimensional vectors** using Ollama embeddings (`all-minilm`).  
3. **Vector Storage**  
- Store generated embeddings in **Astra DB**:  
  - Database: `langflow_test1`  
  - Collection: `pdf`  
4. **Query Processing**  
- Process user questions:  
  - Perform vector similarity search.  
  - Retrieve the top 5 most relevant results.  
5. **Response Generation**  
- Use a **Prompt Template** to combine context and the user's question.  
- Generate context-aware answers with **Llama3.2**.  

## üöÄ Setup Instructions
1. [Get Ollama here](https://ollama.com/) and run the following commands in your terminal:  
   ```bash
   ollama pull llama3.2:latest
   ollama pull all-minilm:latest
   ollama serve  # Base URL: http://localhost:11434
2. [**Install Langflow:**](https://github.com/langflow-ai/langflow)
   ```bash
   !pip install langflow
3. **Import the Pipeline:**
   Open the Langflow UI by running the following command:
   ```bash
   langflow run
   ```
   and load the Vector-Store-RAG.json file.

## üõ†Ô∏è Customization Options
1. **Vector Stores:**
   Replace Astra DB with: Cassandra, Chroma DB, MongoDB Atlas, Pinecone, Cassandra Graph, etc.
   (Update AstraDB nodes in Langflow with your preferred database config)

4. **LLMs:**
   Use alternatives like OpenAI ChatGPT, Anthropic Claude, Google Gemini, Mistral, HuggingFace models, etc.
   (Modify the OllamaModel node in the pipeline)

3. **Embeddings:**
   Swap Ollama for: HuggingFace, OpenAI, NVIDIA, Astra Vectorize, etc.
   (Update the OllamaEmbeddings node configuration)

## üß© Pipeline Components 

| **Component**      | **Role**                                                                 |
|---------------------|-------------------------------------------------------------------------|
| **File Loader**     | Ingests *Shadows of Eldoria.pdf*                                       |
| **Text Splitter**   | Splits text into 500-character chunks with 100-character overlap       |
| **Ollama Embeddings** | Generates 384-dim vectors using `all-minilm:latest`                  |
| **Astra DB**        | Stores vectors and enables similarity search                          |
| **Prompt Template** | Combines context and user question for the LLM                        |
| **Llama3.2**        | Generates answers using retrieved context                             |


## üóÇÔ∏è Pipeline Steps

#### Step 1: Loading the Document
Use the "File" node to upload the document you want to process. In this case, we used `Shadows of Eldoria.pdf`.

#### Step 2: Text Splitting
The "Split Text" node was configured with:
- Chunk size: 500
- Overlap: 100

This ensures efficient processing by dividing the text into manageable chunks.

#### Step 3: Generating Embeddings
The "Ollama Embeddings" node is used to create embeddings for the text chunks, leveraging the `all-minilm` model.

#### Step 4: Data Storage and Retrieval
- Configured the "Astra DB" node to store and retrieve embeddings.
- Defined the database and collection (`langflow_test1` and `pdf`).

#### Step 5: Generative AI
Integrate the "Ollama" node to use the `Llama3.2` model for generating answers with contextual understanding. 





